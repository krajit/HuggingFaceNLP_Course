{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import setup_chat_format\n",
    "\n",
    "# Load model and tokenizer\n",
    "#model_id = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "model_id = \"./sft_output/checkpoint-1000\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set up the chat format with default 'chatml' format\n",
    "#model, tokenizer = setup_chat_format(model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from the HuggingFace Hub\n",
    "dataset = load_dataset(\"philschmid/dolly-15k-oai-style\")\n",
    "dataset = dataset['train'].train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 12008\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 3003\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "In a website browser address bar, what does “www” stand for?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "World Wide Web<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(dataset['train'][0]['messages'],tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ef2d8ae8d0414e8328ce52f718009d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/12008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e610caec13040daaf6871cd3f563a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/12008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3ae1652894445e82cbb715b02e8865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/12008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8739 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01485efa12b347f982730e6ad6032478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/12008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3003c713c75243689a8b0a6c1976094e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1b36cbc95c4188afd1488c502c412d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85048f73b4bc46d0a32d482427e36a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fee06e7f0d44dd8aca979b42b4aadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Configure trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "#trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'Was the new deal successful or a failure to solve the problems of the Great Depression?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'The new deal was a success to solve the problems caused by the Great Depression because he took action quickly, he passed many acts, and he kept the moral up for the people.  Franklin Roosevelt writes \"This is no usolvable prroblem if we face it wisely and courageously.  It can be accomplished in part by direct recruiting by the Government itself, treating the task as we would treat the emergency of a war.\"  Roosevelt is saying that unlike Hoover, he was going to take action on what was happening.  Anything he will do will be treated as if it was war because it was affecting them as much as war would.  The New Deal Legislation shows that Franklin Roosevelt had a god relationship with Congress.  11 new acts were passed in 6 years while Rosevelt was in office.  This helps people get jobs, lower prices, and help people get money they deserve.  Roosevelt says \"This great Nation will endure as it has endured, will revive and will prosper.  This shows that Rosevelt knows how to lead people even in the hard time.  He knew while it was bad, everyone will look to him, so he can\\'t seem scared and he can\\'t kill the moral of the people.  With all of this, Roosevelt was able to lead the country away from the Great Depression and started to get them out of it.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "Was the new deal successful or a failure to solve the problems of the Great Depression?\n",
      "assistant\n",
      "The New Deal was successful in solving the problems of the Great Depression.  The New Deal was a response to the Great Depression.  The New Deal was successful in solving the problems of the Great Depression.  The New Deal was successful in solving the problems of the Great Depression.  The New Deal was successful in solving the problems of the Great Depression.  The New Deal was successful in solving the problems of the Great Depression.  The New Deal was successful in solving the problems\n"
     ]
    }
   ],
   "source": [
    "# talking to a trained model\n",
    "\n",
    "# Let's test the base model before training\n",
    "prompt = \"Was the new deal successful or a failure to solve the problems of the Great Depression?\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was the new deal successful or a failure to solve the problems of the Great Depression?\n",
      "\n",
      "The answer to this question is that the new deal was successful in solving the problems of the Great Depression. The new deal was a step in the right direction\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model_id)\n",
    "\n",
    "# Generate text based on a prompt\n",
    "prompt = \"Was the new deal successful or a failure to solve the problems of the Great Depression?\"\n",
    "output = generator(prompt, max_length=50)\n",
    "\n",
    "# Print the generated text\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
